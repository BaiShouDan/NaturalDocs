/* 
 * Class: GregValure.NaturalDocs.EngineTests.Framework.FileBasedTesting.Tester
 * ____________________________________________________________________________
 * 
 * A base class for unit tests where sample input files are loaded from a folder, output is generated for each one, and 
 * then that output is compared with files containing the expected output.
 * 
 * 
 * Rationale:
 * 
 *		This system is ideal for complex text conversion tests (comments to NDMarkup for example) where hand coding the 
 *		tests and results into unit test code is impractical: the inputs may be very long, and the exact output format may
 *		change over time.  It may also be too unwieldy to determine the expected output by hand.  Rather, the code should
 *		do it and the user should just approve the results.
 *		
 *		This system enables just that.  The inputs are all text files located in a folder.  They can contain as much text as 
 *		necessary.  The output from the conversion is saved to a file in the same folder and compared against an expected
 *		output file.  If they match, the test passes.  If they don't and it fails, the user can compare the expected and actual
 *		output files.  If it's a legitimate bug the code can be fixed.  If the output format has just changed and the actual
 *		output file is acceptible, it can be renamed to become the new expected output file.  This can also be done for the
 *		very first time a test is run and there is no expected output file yet.
 *	
 * 
 * Files:
 * 
 *		> [Test Name] - Input.[extension]
 *		
 *		Any files matching this format will be considered a test.
 *		
 *		If the file contains a line in the format "--- Test" or "--- Test Data", everything above will be considered the <Description>
 *		and everything below the actual test data.  This allows you to explain the test rationale and expected outcomes directly
 *		in the test file.
 *		
 *		> [Test Name] - Expected Output.txt
 *		
 *		The expected output generated for the test.
 *		
 *		> [Test Name] - Actual Output.txt
 *		
 *		The actual output generated by the test.
 *		
 * 
 * Configuration:
 * 
 *		This class will look for a folder called "ND Config" in the Test Data folder.  If it exists, it will use the configuration files 
 *		present in it.  If it doesn't, it will create an empty temporary directory to use and Natural Docs will run using its default 
 *		configuration.
 *		
 * 
 *	 Usage:
 *	 
 *		- Derive a NUnit test fixture from this class.
 *		- Override <Run()> to convert the passed input to the output.
 *		- Create one or more NUnit test functions that call <RunFolder()> to run all the test files in that folder.
 */

// This file is part of Natural Docs, which is Copyright © 2003-2011 Greg Valure.
// Natural Docs is licensed under version 3 of the GNU Affero General Public License (AGPL)
// Refer to License.txt for the complete details


using System;
using System.Collections.Generic;
using System.Text;
using NUnit.Framework;
using GregValure.NaturalDocs.Engine;


namespace GregValure.NaturalDocs.EngineTests.Framework.FileBasedTesting
	{
	public abstract class Tester
		{

		// Group: Functions
		// __________________________________________________________________________

		/* Constructor: Tester
		 */
		public Tester ()
			{
			}


		/* Function: Run
		 * 
		 * Override this function to perform the test and return the output for the passed input.
		 * 
		 * You do not need to worry about catching exceptions unless the test is supposed to trigger them.  Uncaught exceptions
		 * will be handled automatically and cause the test to fail.  If the exception was intended as part of correct operation then 
		 * you must catch it to prevent this.
		 * 
		 * This function should not return null or an empty string as part of a successful test.  Doing so will cause the test to fail.
		 * If a test is supposed to generated no output, return a string such as "test successful" instead.
		 */
		public abstract string Run (string input, string inputFileExtension);


		/* Function: RunFolder
		 * 
		 * Tests all the input files contained in this folder.
		 * 
		 * If you pass a relative path it will take the executing assembly path, skip up until it passes "bin", move into the "Test Data"
		 * subfolder, and then make the path relative to that.  This is because it's meant to be run from a Visual Studio source tree, 
		 * so from C:\Project\bin\debug\EngineTests.dll it will look for C:\Project\Test Data\[test folder].
		 */
		public void RunFolder (Path testFolder)
			{
			// Resolve and validate the test folder

			Path testFolderAsEntered = testFolder;

			if (testFolder.IsRelative)
				{
				string assemblyPath = Path.GetExecutingAssembly();
				int binIndex = assemblyPath.IndexOf("/bin/");

				if (binIndex == -1)
					{  Assert.Fail("Couldn't find bin folder in " + assemblyPath);  }

				testFolder = assemblyPath.Substring(0, binIndex) + "/Test Data/" + testFolder;
				}

			if (System.IO.Directory.Exists(testFolder) == false)
				{  Assert.Fail("Cannot locate test folder " + testFolder);  }


			// Make temporary folders and see if there's a config folder already.

			Path temporaryFolder = testFolder + "/ND Temp";
			
			System.IO.Directory.CreateDirectory(temporaryFolder);
			System.IO.Directory.CreateDirectory(temporaryFolder + "/Input");
			System.IO.Directory.CreateDirectory(temporaryFolder + "/Output");
			System.IO.Directory.CreateDirectory(temporaryFolder + "/Working Data");

			Path configFolder;

			if (System.IO.Directory.Exists(testFolder + "/ND Config"))
				{  configFolder = testFolder + "/ND Config";  }
			else
				{
				configFolder = temporaryFolder + "/Config";
				System.IO.Directory.CreateDirectory(configFolder);
				}


			List<TestResult> testResults = new List<TestResult>();
			int failureCount = 0;


			// INITIALIZE ZE ENGINE!

			Engine.Instance.Create();

			try
				{
				Engine.Instance.Config.ProjectConfigFolder = configFolder;
				Engine.Instance.Config.WorkingDataFolder = temporaryFolder + "/Working Data";

				Engine.Instance.Config.CommandLineConfig.Entries.Add(
					new Engine.Config.Entries.InputFolder(temporaryFolder + "/Input", Engine.Files.InputType.Source)
					);
				Engine.Instance.Config.CommandLineConfig.Entries.Add(
					new Engine.Config.Entries.HTMLOutputFolder(temporaryFolder + "/Output")
					);

				Engine.Errors.ErrorList startupErrors = new Engine.Errors.ErrorList();
				if (!Engine.Instance.Start(startupErrors))
					{
					StringBuilder message = new StringBuilder();
					message.Append("Could not start the Natural Docs engine for testing:");

					foreach (var error in startupErrors)
						{  
						message.Append("\n - ");
						if (error.File != null)
							{  message.Append(error.File + " line " + error.LineNumber + ": ");  }
						message.Append(error.Message);
						}

					Assert.Fail(message.ToString());
					}


				// Iterate through files

				string[] files = System.IO.Directory.GetFiles(testFolder);
				Test test = new Test();

				foreach (string file in files)
					{
					if (file.Contains(" - Input."))
						{
						test.Load(file);

						try
							{  test.ActualOutput = Run(test.Input, test.InputFile.Extension);  }
						catch (Exception e)
							{  test.TestException = e;  }

						test.SaveOutput();  // Even if an exception was thrown.
						testResults.Add(test.GetTestResult());

						if (test.Passed == false)
							{  failureCount++;  }
						}
					}
				}

			finally
				{
				Engine.Instance.Dispose(true);

				try
					{  System.IO.Directory.Delete(temporaryFolder, true);  }
				catch
					{  }
				}


			// Build status message

			if (testResults.Count == 0)
				{
				Assert.Fail("There were no tests found in " + testFolder);
				}
			else if (failureCount > 0)
				{
				StringBuilder message = new StringBuilder();
				message.Append(failureCount.ToString() + " out of " + testResults.Count + " test" + (testResults.Count == 1 ? "" : "s") + 
												  " failed for " + testFolderAsEntered + ':');

				foreach (TestResult testResult in testResults)
					{  
					if (testResult.Passed == false)
						{  message.Append("\n - " + testResult.Name);  }
					}

				Assert.Fail(message.ToString());
				}
			}

		}

	}